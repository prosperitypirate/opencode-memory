# DevMemBench

A coding-assistant memory benchmark for [opencode-memory](../README.md). Evaluates recall quality across 8 developer-specific categories using a 5-phase LLM-as-judge pipeline.

Unlike general benchmarks (LongMemEval, LoCoMo), this dataset is designed around **coding assistant interactions**: architecture decisions, error fixes, tech stack, session continuity across days, and knowledge updates as a project evolves.

---

## Results

> Model: `claude-sonnet-4-6` (judge + answerer) · 40 questions · 10 sessions

### v0.4 — Temporal Grounding (run `149e7d1f`) — **87.5%** ↑ +5pp on avg

Session-continuity jumps from 20% → **60%** as predicted by the issue estimate.

```
tech-stack        ████████████████████ 100%  (5/5)  ✓  stable
architecture      ████████████████████ 100%  (5/5)  ✓  stable
preference        ████████████████████ 100%  (5/5)  ✓  stable
error-solution    ████████████████████ 100%  (5/5)  ✓  stable
knowledge-update  ████████████████████ 100%  (5/5)  ✓  stable
abstention        ████████████████░░░░  80%  (4/5)  ✓
continuity        ████████████░░░░░░░░  60%  (3/5)  ↑  was 20% → +40pp
synthesis         ████████████░░░░░░░░  60%  (3/5)  ⚠  unchanged
─────────────────────────────────────────────────────────────
Overall           87.5%  (35/40)
```

**What the two remaining session-continuity misses reveal:**
- **Q11** ("most recent session"): The system correctly identified S10 (2025-02-05) as the most recent session. However the ground truth targets S09 (2025-02-01) — a dataset inconsistency; S10 was added after the question was written. The recency boost works correctly.
- **Q14** ("January 17 session"): The query is a pure date reference with no semantic content matching the memories from that session (uvicorn, alembic, pytest). Vector search can't retrieve memories by date alone. Would require date-indexed lookup, not vector similarity.

> **Note on variance:** LLM-based benchmarks are non-deterministic even at temperature 0. Two sources: (1) **chunk leakage** — raw source chunks for post-migration memories contain old technology names in context; (2) **contradiction detection** — the LLM call identifying superseded memories may not fire identically each run.

### v0.3 — Relational Versioning (runs `cb9f84d0`, `d6af0edd`) — **82.5% avg** ↑ +30pp

Knowledge-update is **consistently 100%** across both runs — the core fix for stale memories is working. Other categories show ±10pp run-to-run variance from non-deterministic LLM extraction and contradiction detection (see note below).

```
architecture      ████████████████████ 100%        ✓  stable
preference        ████████████████████ 100%        ✓  stable
knowledge-update  ████████████████████ 100%        ✓  stable  was 40% → +60pp
abstention        ████████████████████ 100%        ✓  stable
tech-stack         80%–80%            ████████████████░░░░    stable
error-solution    80%–100%            ████████████████░░░░    varies
cross-synthesis   40%–80%             ████████–████████████   varies
session-cont.     20%–40%             ████–████               varies
─────────────────────────────────────────────────────────────
Overall           77.5%–87.5%  (avg 82.5%)
```

### v0.2 — Hybrid Search (run `e2052c0f`) — **85.0%** ↑ +32.5pp

```
tech-stack        ████████████████████ 100%  (5/5)  ✓
architecture      ████████████████████ 100%  (5/5)  ✓
preference        ████████████████████ 100%  (5/5)  ✓
error-solution    ████████████████████ 100%  (5/5)  ✓  was 0%  → +100pp
knowledge-update  ████████████████████ 100%  (5/5)  ✓  was 40% → +60pp
abstention        ████████████████░░░░  80%  (4/5)  ✓
cross-synthesis   ████████████░░░░░░░░  60%  (3/5)  ⚠  was 20% → +40pp
session-cont.     ████████░░░░░░░░░░░░  40%  (2/5)  ⚠  was 20% → +20pp
```

### v0.1 — Baseline (run `ab3bff99`) — **52.5%**

```
preference        ████████████████████ 100%  (5/5)  ✓
tech-stack        ████████████████░░░░  80%  (4/5)  ✓
architecture      ████████████████░░░░  80%  (4/5)  ✓
abstention        ████████████████░░░░  80%  (4/5)  ✓
knowledge-update  ████████░░░░░░░░░░░░  40%  (2/5)  ⚠
session-cont.     ████░░░░░░░░░░░░░░░░  20%  (1/5)  ✗
cross-synthesis   ████░░░░░░░░░░░░░░░░  20%  (1/5)  ✗
error-solution    ░░░░░░░░░░░░░░░░░░░░   0%  (0/5)  ✗
```

---

## Root Cause Analysis

The 4 failing categories share 3 concrete root causes, each of which maps directly to a Supermemory architectural technique.

### 1. Memory summaries lose technical specifics → `error-solution` 0%

The backend LLM extracts a short atomic fact per message. Specific values (exact error messages, config numbers, function names) are compressed into prose and lost:

```
Session said:   max_connections=50, socket_connect_timeout=5, socket_timeout=5
Memory stored:  "Always check Redis connection pool settings first for timeout spikes"
```

The search found the right memory (score 55–73%) but the answering model had no access to the actual values, so it either hedged ("I don't know the specific settings") or hallucinated plausible numbers.

**Supermemory's fix:** Hybrid search — semantic search on atomic memories to locate the right result, then inject the **original source chunk** (raw conversation text) into the answer context. The LLM reads the precise code/config from the chunk, not from the compressed memory summary.

---

### 2. Stale memories are never superseded → `knowledge-update` 40%, `cross-session-synthesis` 20%

The project migrated from SQLAlchemy → Tortoise ORM across sessions. Both memories coexist in the store with equal weight:

```
S01 (Jan 15):  "Use SQLAlchemy ORM and Alembic for migrations"   ← stale
S05 (Jan 20):  "Switched to Tortoise ORM and Aerich"             ← current
```

When asked "what ORM is used?", the LLM received both and guessed wrong (SQLAlchemy). Q31 and Q32 both failed this way — they explicitly asked about the current state but got the old state.

**Supermemory's fix:** Relational versioning — when ingesting a memory that contradicts an existing one, create an `updates` relationship. Old memories are excluded from search results or ranked lower. This creates a version chain instead of a flat pile of facts.

**Status: ✅ Implemented (PR #21).** After each ADD, a similarity search finds candidate memories within cosine distance 0.5, then an LLM call identifies which are factually superseded. Confirmed entries are marked `superseded_by = <new_id>` and excluded from all reads. `knowledge-update` is consistently **100%** across runs.

---

### 3. Search recall fails for recency queries → `session-continuity` 20%

Q11 ("What was the most recent session focused on?") and Q12/Q14 returned 0 results. The queries used natural-language recency framing ("most recent", "last time we worked on X") that didn't match any stored memory text semantically.

The backend has no concept of time in retrieval — every memory is searched by cosine similarity alone, with no temporal weighting or metadata filtering.

**Supermemory's fix:** Dual-layer timestamps — `documentDate` (when the conversation happened) and `eventDate` (when the described event occurred). Retrieval can filter or boost by recency. Their answering prompt also explicitly instructs the LLM to use temporal context when reasoning about "recent" queries.

---

## Comparison with Supermemory

Supermemory published [LongMemEval_s](https://supermemory.ai/research) results (general conversational memory, 500 questions):

| System | Overall |
|---|---|
| Full-context GPT-4o | 60.2% |
| Zep (GPT-4o) | 71.2% |
| **Supermemory (GPT-4o)** | **81.6%** |
| **opencode-memory v0.4 (claude-sonnet-4-6)** | **87.5%** ← after temporal grounding |
| Supermemory (Gemini 2.5 Pro) | 85.2% |

Note: different datasets (coding context vs. general conversational), so scores are not directly comparable. What is comparable is **which techniques drive improvement**:

| Supermemory technique | Their uplift | v0.1 gap | v0.4 result |
|---|---|---|---|
| Hybrid search (memory → source chunk) | +22.9% temporal | error-solution 0% | **100%** ✓ (#19) |
| Relational versioning (`updates` link) | +6.2% knowledge-update | knowledge-update 40%, synthesis 20% | **knowledge-update 100%** ✓ (#21) |
| Temporal grounding (documentDate + eventDate) | +22.9% temporal | session-continuity 20% | **60%** ✓ (#18) |
| Contextual memory extraction | Reduces ambiguity | architecture 1 miss, abstention 1 miss | unchanged |

The remaining improvements below directly address each open technique.

---

## Improvement Roadmap

These are sequenced by estimated impact. Each is a separate backend PR.

### ~~Priority 1 — Hybrid search: store and return source chunks~~ ✅ Done (PR #19, +32.5pp)

**Actual results: error-solution 0% → 100%, knowledge-update 40% → 100%, cross-synthesis 20% → 60%, session-continuity 20% → 40%**

Backend stores the original conversation text as a `chunk` field alongside each extracted memory. `POST /memories/search` returns both. The benchmark answering prompt injects the chunk as source context so the LLM reads exact values (config numbers, error strings, function names) rather than compressed summaries.

Plugin also updated: dual-scope semantic search (user + project) at session start, with raw chunk snippets injected into `[MEMORY]` for hits ≥55% similarity.

### ~~Priority 2 — Relational versioning: supersede stale memories~~ ✅ Done (PR #21)

**Actual results: knowledge-update consistently 100% (up from 40%). Overall range 77.5–87.5% across runs (avg 82.5%). Variance from non-deterministic LLM extraction and chunk leakage in synthesis questions.**

After each new ADD, a candidate search (cosine distance ≤ 0.5) finds semantically related existing memories, then an LLM call determines which are factually superseded by the new memory. Confirmed entries are marked `superseded_by = <new_id>` and excluded from `POST /memories/search` and `GET /memories` by default. Auto-migration adds the field to existing tables on startup. Benchmark cleanup updated to use `?include_superseded=true` so no orphaned rows are left behind.

### ~~Priority 3 — Temporal metadata in search and prompts~~ ✅ Done (PR #18, +40pp session-continuity)

**Actual results: session-continuity 20% → 60%. Overall 87.5% (35/40).**

Two-part implementation:
1. **Date in search results** — backend now extracts `metadata.date` (session date) from each memory's `metadata_json` and returns it as a top-level `date` field in `POST /memories/search`. The benchmark answer prompt shows `date: YYYY-MM-DD` alongside each memory so the answering LLM can reason temporally ("2025-02-01 is the most recent").
2. **Per-category recency blending** — `POST /memories/search` accepts `recency_weight` (default 0). Score = `(1 - w) * semantic + w * exp(-0.1 * days_from_newest)`. The benchmark uses weight **0.5** for `session-continuity` only (temporal queries) and **0.0** for all other categories (pure semantic). Uses `metadata.date` — the session date — not `created_at` (ingestion timestamp), so ordering is meaningful even when all sessions are ingested in a single run.

Remaining session-continuity gap (40%) is split between a dataset inconsistency (Q11) and date-only queries with no semantic content (Q14) — see v0.4 notes above.

---

## Dataset

- **10 sessions** — synthetic `ecommerce-api` project (FastAPI + PostgreSQL + Redis + Stripe)
- **40 questions × 8 categories** — 5 per category
- **Isolated per run** — `bench_devmem_{runId}` tag; real memories never touched
- **Project evolution** — sessions span Jan–Feb 2025, including ORM migration (SQLAlchemy → Tortoise ORM) and Stripe integration, testing whether stale knowledge is overwritten

### Categories

| Category | Tests |
|---|---|
| `tech-stack` | Language, framework, infra choices |
| `architecture` | System design, component relationships, API contracts |
| `session-continuity` | What happened in the most recent/previous session |
| `preference` | Developer style, tool preferences, conventions |
| `error-solution` | Specific bugs fixed with exact details |
| `knowledge-update` | Updated facts superseding older ones |
| `cross-session-synthesis` | Patterns spanning multiple sessions |
| `abstention` | Correctly declining when info was never stored |

---

## Running locally

### Prerequisites

- [Bun](https://bun.sh) ≥ 1.0
- opencode-memory backend + frontend running via Docker Compose (see root README)
- An Anthropic API key (Claude is used for both answering and judging)

### First-time setup

```bash
# 1. Start the backend (from repo root)
docker compose up -d

# 2. Install benchmark dependencies
cd benchmark
bun install

# 3. Configure environment
cp .env.example .env.local
# Edit .env.local and set:
#   ANTHROPIC_API_KEY=sk-ant-...
#   MEMORY_BACKEND_URL=http://localhost:8020   ← default, change only if needed
```

### Running the benchmark

```bash
bun run bench run
```

That's it. Every run automatically:
1. **Opens the live dashboard** at `http://localhost:4242` in your browser
2. Streams live progress through Ingest → Search → Answer → Evaluate phases
3. Prints the final score table in the terminal
4. Cleans up test memories from the backend when done (~5 min total)

### Other commands

```bash
bun run bench run -r my-run         # named run — safe to interrupt and resume
bun run bench run --no-cleanup      # keep memories in backend after run (for debugging)
bun run bench run --limit 10        # smoke test — runs only the first 10 questions (~1 min)
bun run bench serve -r <id>         # re-open the dashboard for a completed run
bun run bench status -r <id>        # print checkpoint status for a run
bun run bench list                  # list all past runs with scores
```

### Pipeline

```
ingest    → POST sessions to backend (isolated by runTag — real memories never touched)
search    → semantic search per question, saves top-8 results
answer    → LLM generates answer from retrieved context only
evaluate  → LLM-as-judge: correct (1) or incorrect (0) vs ground truth
report    → aggregate by category, print table, save report.json
cleanup   → delete all test memories for this run
```

Checkpointed after each phase — if interrupted, re-run the same command with `-r <id>` to resume from where it left off.

### Environment variables

| Variable | Default | Description |
|---|---|---|
| `MEMORY_BACKEND_URL` | `http://localhost:8020` | Backend URL |
| `ANTHROPIC_API_KEY` | — | Required (Claude judge + answerer) |
| `OPENAI_API_KEY` | — | Alternative if using OpenAI models |
| `JUDGE_MODEL` | `claude-sonnet-4-6` | Override judge model |
| `ANSWERING_MODEL` | `claude-sonnet-4-6` | Override answering model |

Run output is saved to `data/runs/<run-id>/` (gitignored).
