# codexfi E2E Test Suite

Fully autonomous end-to-end tests for the `codexfi` plugin. The agent (not a human) creates isolated project directories, spawns `opencode run` sessions, talks to the agent, inspects the memory backend, and reports pass/fail — zero user interaction required.

> **Note:** As of the plugin embedded rewrite, the test harness uses the embedded LanceDB store
> directly (via `plugin/src/store.ts` and `plugin/src/db.ts`) instead of making HTTP calls
> to the Docker backend. The Docker backend is no longer required for E2E testing.

## Prerequisites

**1. Plugin built**
```bash
cd plugin && bun run build
```

**2. OpenCode CLI installed**
```bash
bun install -g opencode-ai
# Verify: opencode --version  (1.2.10+)
```

**3. Plugin configured in `~/.config/opencode/opencode.json`**
```json
{
  "plugin": ["file:///path/to/codexfi/plugin/dist/index.js"]
}
```

**4. API keys set in environment**
- `VOYAGE_API_KEY` — required for embeddings (voyage-code-3)
- `ANTHROPIC_API_KEY` — required for extraction (claude-haiku-4-5)
- An AI provider for the OpenCode agent (e.g. Anthropic API key in config)

## Running

```bash
cd testing
bun install       # first time only
bun run test      # runs all 12 scenarios
```

Output is printed to stdout with ANSI colours. Each run is also saved to `results/` (gitignored).

After each scenario completes, the test harness **automatically deletes all memories it created** from the backend — keeping the memory store clean between runs.

To run a single scenario:
```bash
bun run test:scenario 07       # single scenario
bun run test:scenario 07,08    # multiple scenarios
```

## Latest run results (2026-02-25) — plugin embedded rewrite

Full run against `plugin/` (embedded LanceDB, no Docker backend). Extraction via xAI Grok (`grok-4-fast-non-reasoning`).

```
PASS  01  Cross-Session Memory Continuity
PASS  02  README-Based Project-Brief Seeding
PASS  03  Transcript Noise Guard
PASS  04  Project Brief Always Present
PASS  05  Memory Aging
PASS  06  Existing Codebase Auto-Init
PASS  07  Enumeration Hybrid Retrieval
PASS  08  Cross-Synthesis (isWideSynthesis)
WARN  09  maxMemories=20 Under Load           ← 8/10 assertions, K=20 margin under 41 memories
PASS  10  Knowledge Update / Superseded
PASS  11  System Prompt Memory Injection
PASS  12  Multi-Turn Per-Turn Refresh

11/12 PASS (scenario 09 is known K=20 margin, not a regression)
```

### Previous results (2026-02-22) — plugin-v1 with Docker backend

Full run against plugin build from PR #44 (`feature/mid-session-retrieval` branch):

```
PASS  01  Cross-Session Memory Continuity          12.7s
PASS  02  README-Based Project-Brief Seeding       19.5s
PASS  03  Transcript Noise Guard                   19.1s
PASS  04  Project Brief Always Present             17.9s
PASS  05  Memory Aging                             38.7s
PASS  06  Existing Codebase Auto-Init              68.7s
PASS  07  Enumeration Hybrid Retrieval             25.5s
PASS  08  Cross-Synthesis (isWideSynthesis)        37.6s
PASS  09  maxMemories=20 Under Load                96.3s
PASS  10  Knowledge Update / Superseded            65.5s
PASS  11  System Prompt Memory Injection           18.2s
PASS  12  Multi-Turn Per-Turn Refresh              45.3s

12/12 PASS  —  Total: ~465s (~7.8 min)
```

### Observations and known issues

**Scenario 04 — project-brief type not extracted without README**
The `project-brief` memory type is never saved when there's no README. The agent extracts `tech-context` and `progress` instead. The scenario still passes because memory recall works — but the `project-brief` count assertion is diagnostic only (non-blocking). Root cause: `seedProjectBrief` likely only fires on `triggerSilentAutoInit` (README path), not on conversation-only sessions.

**Scenario 03 — assertion updated (not a bug)**
The original assertion checked `s2.text` for the project name "ferrite-api". The LLM responded correctly ("a Rust web service") without always naming it. Changed to check the backend memory content directly — more reliable.

**Scenario 09 — memory count is non-deterministic**
The xAI extractor consolidates facts differently each run — 6 short sessions produced 5 memories once, 18 another time. The scenario uses rich, detailed session messages (8 sessions) to ensure consistent ≥8 memory count. This mirrors the same ingest nondeterminism seen in the benchmark.

## Known issue: OpenCode Desktop app interference

If the OpenCode desktop app is running, it sets `OPENCODE_SERVER_PASSWORD`, `OPENCODE_SERVER_USERNAME`, and `OPENCODE_CLIENT` in your shell environment. The `opencode run` CLI inherits these and its internal server then requires Basic Auth — but run-mode sends no auth headers, causing every CLI session to fail silently.

**The test harness already handles this automatically** via `cleanEnv()` in `src/opencode.ts`, which strips those three variables before spawning each `opencode run` process. You do not need to close the desktop app.

If you run `opencode run` manually from a terminal where the desktop app is active and see unexpected failures, unset those vars first:
```bash
env -u OPENCODE_SERVER_PASSWORD -u OPENCODE_SERVER_USERNAME -u OPENCODE_CLIENT \
  opencode run "your message" --dir /path/to/project -m anthropic/claude-sonnet-4-6
```

This is a known bug in `opencode` v1.2.10 — tracked at https://github.com/anomalyco/opencode/issues/14532.

## Scenarios

| # | Name | What it tests |
|---|------|---------------|
| 01 | Cross-Session Memory Continuity | Auto-save fires after session end; session 2 recalls facts from session 1 |
| 02 | README-Based Project-Brief Seeding | `triggerSilentAutoInit` reads README on first session; project-brief memory is created and recalled in session 2 |
| 03 | Transcript Noise Guard | Saved memories contain no raw `[user]`/`[assistant]` transcript lines; memory recall works across sessions |
| 04 | Project-Brief Always Present | Memories accumulate from conversation even without README; session 2 recalls project facts |
| 05 | Memory Aging | Backend replaces older `progress` memories with newest; only 1 survives across 3 sessions |
| 06 | Existing Codebase Auto-Init | `triggerSilentAutoInit` reads real project files (package.json, tsconfig, src/) on first open; memories are created without any conversation seeding |
| 07 | Enumeration Hybrid Retrieval | `types[]` param fires for "list all preferences" queries; answer covers preferences seeded across multiple sessions, not just the most recent |
| 08 | Cross-Synthesis (isWideSynthesis) | "across both projects" heuristic fires; answer synthesises facts from two separate project memory namespaces |
| 09 | maxMemories=20 Under Load | With >10 memories stored, facts from early sessions still recalled — confirms K=20 retrieval depth |
| 10 | Knowledge Update / Superseded | After ORM migration, agent answers with the new ORM (Tortoise), not the stale one (SQLAlchemy); backend reflects superseded state |
| 11 | System Prompt Memory Injection | [MEMORY] block is injected via `system.transform` into the system prompt (not as a synthetic message part); agent references seeded facts |
| 12 | Multi-Turn Per-Turn Refresh | 6-turn conversation via `opencode serve`; per-turn semantic refresh surfaces topic-relevant memories as the user switches topics mid-session |

## Architecture

```
testing/
├── src/
│   ├── runner.ts          — entry point, runs all scenarios sequentially
│   │                        (refreshes LanceDB table before cleanup to fix delete lock contention)
│   ├── opencode.ts        — spawns opencode serve with per-directory server cache
│   │                        (opencode run exits before async handlers complete; serve stays alive)
│   ├── memory-api.ts      — uses embedded LanceDB store directly (plugin/src/store.ts + db.ts)
│   │                        (replaces HTTP calls to Docker backend; calls db.refresh() before reads)
│   ├── report.ts          — ANSI result formatting
│   └── scenarios/
│       ├── 01-cross-session.ts
│       ├── 02-readme-seeding.ts
│       ├── 03-transcript-noise.ts
│       ├── 04-project-brief-always.ts
│       ├── 05-memory-aging.ts
│       ├── 06-existing-codebase.ts
│       ├── 07-enumeration-retrieval.ts
│       ├── 08-cross-synthesis.ts
│       ├── 09-max-memories.ts
│       ├── 10-knowledge-update.ts
│       ├── 11-system-prompt-injection.ts
│       └── 12-multi-turn-refresh.ts
├── results/               — gitignored; JSON output of each run
├── package.json
└── tsconfig.json
```

### Key changes for plugin

1. **`memory-api.ts` rewired** — imports `store.*` and `db.*` from `plugin/src/` directly instead of making HTTP calls to `localhost:8020`. Calls `db.refresh()` before reads to pick up writes from the `opencode serve` child process (LanceDB caches table state).

2. **`opencode.ts` uses server mode** — `opencode run` exits before async plugin event handlers (auto-save, extraction) complete. Switched to `opencode serve` with per-directory server caching so the plugin process stays alive for extraction to finish. `waitForMemories(dir, N, 30_000)` polls until data appears.

3. **`runner.ts` refreshes table** — calls `db.refresh()` before cleanup deletes to fix lock contention when the serve process is still holding a stale table handle.

4. **Scenario 12 calls `addMemoryDirect()`** — this imports `store.ingest()` directly in the test runner process (not in a child `opencode serve` process), so extraction provider/model config must be correct in the built `dist/index.js`. Different config lifecycle than scenarios 01-11.

## How `opencode run` is used

Most scenarios use **single-shot mode** via `opencode run`:
```bash
opencode run "<message>" --dir <isolated-tmp-dir> -m anthropic/claude-sonnet-4-6 --format json
```

`--format json` emits one JSON event per line. The test harness parses these to extract:
- Session ID
- Full text response (concatenated text parts)
- Exit code and timing

Each scenario gets its own isolated `createTestDir()` directory under `/private/tmp/oc-test-<name>-<uuid>`, so tests never share state.

### Persistent server mode (`opencode serve`)

Scenario 12 uses **persistent server mode** for true multi-turn testing. Each `opencode run` invocation is a separate process — the plugin's in-memory session caches reset between runs, making it impossible to test per-turn refresh (turns 2+). `opencode serve` keeps a single plugin process alive across turns.

The test harness provides helpers in `opencode.ts`:
- `startServer(dir)` — spawns `opencode serve --dir <dir>`, waits for the HTTP API to become ready
- `createSession(port, model)` — `POST /session` to create a new session
- `sendServerMessage(port, sessionId, message)` — `POST /session/:id/message` to send a user message and collect the streamed response
- `deleteSession(port, sessionId)` — `DELETE /session/:id` to clean up
- `stopServer(handle)` — kills the server process

The `model` field in the server API must be an object `{ providerID, modelID }`, not a string like `"anthropic/claude-sonnet-4-6"`.

### Direct embedded store seeding

For deterministic test setup, `memory-api.ts` provides:
- `addMemoryDirect(projectTag, content, type?)` — calls `store.ingest()` directly to seed a memory (runs through the LLM extractor in the test runner process)
- `searchMemories(projectTag, query, limit?)` — calls `store.search()` directly to verify semantic search results
- `getMemoriesForDir(dir)` — calls `store.list()` for all memories under a project tag
- `waitForMemories(dir, count, timeoutMs)` — polls `store.list()` until `count` memories appear (used for async auto-save)

## Memory tag computation

The plugin identifies each project by hashing the absolute directory path:
```
projectTag = "opencode_project_" + sha256(directory)[:16]
```

The test harness replicates this logic in `memory-api.ts:projectTagForDir()` to query the backend for exactly the memories that the plugin would have written.
