<div align="center">

# opencode-memory

**Fully self-hosted, autonomous persistent memory for [OpenCode](https://opencode.ai) AI agents.**

The agent learns from every session automatically ‚Äî no commands, no manual saves, no cloud.

<br/>

[![License: MIT](https://img.shields.io/badge/License-MIT-F7DF1E?style=flat)](https://opensource.org/licenses/MIT)
[![TypeScript](https://img.shields.io/badge/TypeScript-5.7-3178C6?style=flat&logo=typescript&logoColor=white)](https://www.typescriptlang.org/)
[![Python](https://img.shields.io/badge/Python-3.13-3776AB?style=flat&logo=python&logoColor=white)](https://www.python.org/)
[![Bun](https://img.shields.io/badge/Bun-Runtime-FBF0DF?style=flat&logo=bun&logoColor=black)](https://bun.sh/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.115-009688?style=flat&logo=fastapi&logoColor=white)](https://fastapi.tiangolo.com/)
[![Next.js](https://img.shields.io/badge/Next.js-15-000000?style=flat&logo=nextdotjs&logoColor=white)](https://nextjs.org/)
[![Docker](https://img.shields.io/badge/Docker-Compose-2496ED?style=flat&logo=docker&logoColor=white)](https://www.docker.com/)
[![LanceDB](https://img.shields.io/badge/LanceDB-Vector_DB-CF3CFF?style=flat)](https://lancedb.com/)
[![xAI Grok](https://img.shields.io/badge/xAI-Grok-000000?style=flat&logo=x&logoColor=white)](https://x.ai/)
[![Anthropic](https://img.shields.io/badge/Anthropic-Haiku-D97706?style=flat)](https://anthropic.com)
[![Google Gemini](https://img.shields.io/badge/Google-Gemini-4285F4?style=flat&logo=google&logoColor=white)](https://ai.google.dev/)
[![Voyage AI](https://img.shields.io/badge/Voyage_AI-Code_Embeddings-5B6BF5?style=flat)](https://www.voyageai.com/)
[![Self-Hosted](https://img.shields.io/badge/Self--Hosted-100%25_Local-22C55E?style=flat&logo=homeassistant&logoColor=white)](https://github.com/prosperitypirate/opencode-memory)
[![OpenCode Plugin](https://img.shields.io/badge/OpenCode-Plugin-FF6B35?style=flat)](https://opencode.ai)

</div>

<div align="center">

![opencode-memory dashboard](.github/assets/dashboard.png)

</div>

---

## What is this?

OpenCode starts every session from scratch. No memory of past decisions, established patterns, previous mistakes, or project context. Every session, you repeat yourself.

**opencode-memory fixes this permanently.**

After every assistant turn, the conversation is automatically analysed. Key facts are extracted and stored locally on your machine. At the start of every new session, relevant memories are silently injected into the agent's context. From the agent's perspective, it simply *remembers* ‚Äî across days, weeks, and projects.

**The agent never thinks about saving memories. It just happens.**

---

## Vision

Memory should be infrastructure, not a ritual.

Today, every AI coding session starts from zero. Tools like Cline's Memory Bank made this disciplined ‚Äî but still require `/load` at the start of each session and `/save` at the end. The agent can't remember without your help. You are the memory system.

**This is the wrong model.** Memory should operate entirely beneath the agent ‚Äî invisible, automatic, never requiring a command.

OpenCode makes something remarkable possible: a [plugin hook architecture](https://opencode.ai/docs) that fires on every session start, every assistant turn, and every compaction event. These hooks are the foundation. They allow memory to be captured and injected with complete transparency ‚Äî the agent receives context it never explicitly asked for, and the user issues no commands they didn't intend.

The result is a new category: **infrastructure-level memory.**

The broader vision goes further:

- **Any agent, any tool** ‚Äî the memory backend is a standalone local API. Any editor plugin, any agent framework, can query it. The OpenCode plugin is one implementation. VS Code, Cursor, Zed, and others can follow the same pattern.
- **Zero user overhead** ‚Äî memory is captured automatically via hooks after every turn. The user never issues a command, never updates a file, never thinks about memory management.
- **Private by default** ‚Äî everything lives on your machine. No cloud sync, no accounts, no telemetry.
- **Self-improving** ‚Äî memories deduplicate, contradict, condense, and age automatically. The corpus becomes more accurate over time, not more bloated.

The goal is not a better memory bank.

**It is the end of the memory bank as a concept** ‚Äî replaced by invisible memory infrastructure that any agent, in any tool, simply inherits when it starts work.

---

## Features

- **Fully automatic** ‚Äî memories save after every assistant turn with zero user action
- **Multi-provider extraction** ‚Äî choose between xAI Grok (fastest), Anthropic Haiku (most consistent), or Google Gemini via a single env var
- **100% local** ‚Äî LanceDB runs embedded in-process; all data lives in a Docker volume on your machine
- **Always-fresh context** ‚Äî the `[MEMORY]` block is injected into the system prompt on every LLM call via `system.transform` hook, not as a one-time message part ‚Äî zero token accumulation, rebuilt fresh each turn
- **Per-turn semantic refresh** ‚Äî on every user message, a semantic search updates the "Relevant to Current Task" section to match the current conversation topic ‚Äî the agent always sees context aligned to what you're asking about right now
- **Compaction-proof** ‚Äî memory lives in the system prompt, which is never compacted ‚Äî long sessions never lose project context, no matter how many turns
- **Code-optimised embeddings** ‚Äî Voyage `voyage-code-3` is purpose-built for code and technical content
- **Typed memory system** ‚Äî memories are classified (`architecture`, `error-solution`, `preference`, `progress`, etc.) and injected in structured blocks
- **Hybrid search** ‚Äî semantic search on atomic memory facts for retrieval, raw source chunk injected into context for exact values (config numbers, error strings, function names)
- **Smart deduplication** ‚Äî cosine similarity check prevents duplicate memories from accumulating
- **Relational versioning** ‚Äî when new memories contradict existing ones, stale entries are automatically marked superseded and excluded from future retrieval
- **Memory aging** ‚Äî session summaries auto-condense into compact learned patterns over time; only the latest `progress` entry survives
- **Temporal grounding** ‚Äî search results carry their session date; recency blending can optionally boost recent memories for time-sensitive queries
- **Project + user scope** ‚Äî separate namespaces for project-specific knowledge vs. cross-project personal preferences
- **Explicit save support** ‚Äî say "remember this" and the agent immediately stores it
- **Web dashboard** ‚Äî live activity feed, cost tracking, per-project memory browser at `http://localhost:3030`
- **Privacy filter** ‚Äî wrap content in `<private>...</private>` tags to exclude it from extraction

---

## How it compares

| | **opencode-memory** | Cline memory bank | [Supermemory](https://supermemory.ai) plugin | [mem0](https://mem0.ai) fork |
|---|---|---|---|---|
| **Storage** | Local (LanceDB embedded) | Markdown files in repo | Supermemory cloud | Local (Qdrant) |
| **Embeddings** | Voyage `voyage-code-3` | None | Cloud | OpenAI `text-embedding-3-large` |
| **Extraction** | Multi-provider (xAI / Anthropic / Google) | LLM via `/save` (manual) | Cloud | OpenAI `gpt-4o` |
| **User action required** | ‚ùå Zero | ‚úÖ `/load` + `/save` each session | ‚ùå Zero | ‚ùå Zero |
| **Data privacy** | ‚úÖ 100% local | ‚úÖ 100% local | ‚ùå Cloud | ‚úÖ Local |
| **Code-optimised embeddings** | ‚úÖ | ‚ùå | ‚ùå | ‚ùå |
| **Fully automatic saves** | ‚úÖ Every turn | ‚ùå Manual | ‚ùå Keyword-only | ‚ùå Keyword-only |
| **Semantic retrieval** | ‚úÖ | ‚ùå Full file dump | ‚úÖ | ‚úÖ |
| **Session token overhead** | ~3K tokens (system prompt, zero accumulation) | Up to 50K tokens | Varies | Varies |
| **Survives compaction** | ‚úÖ System prompt injection | ‚ùå | ‚ùå | ‚ùå |
| **Per-turn refresh** | ‚úÖ Semantic refresh every turn | ‚ùå | ‚ùå | ‚ùå |

---

## Quick Start

### Prerequisites

| Tool | Purpose | Install |
|------|---------|---------|
| [Docker Desktop](https://www.docker.com/products/docker-desktop/) | Runs the memory server and dashboard | [docker.com](https://www.docker.com/products/docker-desktop/) |
| [Bun](https://bun.sh) | Builds the OpenCode plugin | `curl -fsSL https://bun.sh/install \| bash` |
| [OpenCode](https://opencode.ai) | The AI coding agent this extends | `npm i -g opencode-ai` |
| Extraction API key | Memory extraction ‚Äî pick one provider (see below) | [console.x.ai](https://console.x.ai) ¬∑ [console.anthropic.com](https://console.anthropic.com) ¬∑ [aistudio.google.com](https://aistudio.google.com/apikey) |
| Voyage AI API key | Code embeddings | [voyageai.com](https://www.voyageai.com) ‚Äî free tier available |

---

### Step 1 ‚Äî Clone and build the plugin

```bash
git clone https://github.com/prosperitypirate/opencode-memory
cd opencode-memory/plugin
bun install
bun run build
```

This compiles the TypeScript OpenCode plugin into `plugin/dist/`.

---

### Step 2 ‚Äî Add your API keys

```bash
cd ..                    # back to repo root
cp .env.example .env
```

Open `.env` and add your API keys:

```env
# ‚îÄ‚îÄ Extraction provider ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# "anthropic" ‚Äî Claude Haiku 4.5 ¬∑ default ¬∑ most consistent (93.5% avg benchmark)
# "xai"       ‚Äî Grok 4.1 Fast ¬∑ fastest ¬∑ higher run-to-run variance
# "google"    ‚Äî Gemini 3 Flash ¬∑ native JSON mode
EXTRACTION_PROVIDER=anthropic

# API key for your chosen provider (only one required)
ANTHROPIC_API_KEY=sk-ant-...  # https://console.anthropic.com/settings/keys
# XAI_API_KEY=                # https://console.x.ai
# GOOGLE_API_KEY=             # https://aistudio.google.com/apikey

# Voyage AI API key ‚Äî used for code embeddings (voyage-code-3)
# Get yours at: https://www.voyageai.com  (free tier available)
VOYAGE_API_KEY=pa-...
```

To switch providers later, change `EXTRACTION_PROVIDER` and ensure the matching API key is set, then restart: `docker compose up -d`.

> **Note:** The Docker container reads keys directly from `.env` via `env_file`. Do not export these in your shell ‚Äî a shell environment variable will override the file and the server won't pick up your key.

---

### Step 3 ‚Äî Start the memory server and dashboard

```bash
docker compose up -d
```

This starts two containers:

| Container | Port | Purpose |
|-----------|------|---------|
| `opencode-memory-backend` | `8020` | FastAPI + LanceDB memory server |
| `opencode-memory-frontend` | `3030` | Next.js web dashboard |

Verify the backend is ready:

```bash
curl http://localhost:8020/health
# {"status":"ok","ready":true}
```

Open the dashboard: **[http://localhost:3030](http://localhost:3030)**

Data persists in the `opencode-memory-data` Docker named volume across restarts.

---

### Step 4 ‚Äî Register the plugin with OpenCode

Run the installer from the `plugin/` directory:

```bash
cd plugin
node dist/cli.js install
```

The installer automatically:
- Adds the plugin to `~/.config/opencode/opencode.json`
- Creates a `/memory-init` slash command

> **Manual alternative:** If you prefer, edit `~/.config/opencode/opencode.json` directly and add the absolute path to the `plugin/` subdirectory:
> ```json
> {
>   "$schema": "https://opencode.ai/config.json",
>   "plugin": [
>     "file:///absolute/path/to/opencode-memory/plugin"
>   ]
> }
> ```

---

### Step 5 ‚Äî Add agent instructions (recommended)

The plugin works without this step ‚Äî memories save and inject automatically. But adding agent instructions to `~/.config/opencode/AGENTS.md` significantly improves behavior: the agent understands what the `[MEMORY]` block is, uses the `memory` tool correctly, never announces memory operations, and proactively searches for relevant context.

Create (or append to) `~/.config/opencode/AGENTS.md` with the following:

<details>
<summary>Click to expand the recommended AGENTS.md snippet</summary>

````markdown
# Memory System

You have a **persistent, self-hosted memory system** that works automatically in the background. It uses LanceDB + Voyage AI embeddings, running locally at `http://localhost:8020`.

## How it works (fully automatic ‚Äî no user action needed)

**On every LLM call**, a `[MEMORY]` block is rebuilt and injected into the system prompt via `system.transform` hook. It contains:
- **Project Brief** ‚Äî what the project is, its purpose (includes project-config)
- **Architecture** ‚Äî system design, component structure
- **Tech Context** ‚Äî stack, tools, languages, dependencies
- **Product Context** ‚Äî features, goals, product decisions
- **Progress & Status** ‚Äî current state, what's done, what's next
- **Last Session** ‚Äî summary of the previous conversation
- **User Preferences** ‚Äî personal cross-project preferences
- **Relevant to Current Task** ‚Äî semantically matched memories with similarity % and date `[XX%, YYYY-MM-DD]`

**On new projects** (zero memories), the plugin silently reads `README.md`, `package.json`, `docker-compose.yml`, etc. and extracts memories automatically ‚Äî completely invisible to the user.

**After every assistant turn**, the plugin extracts atomic typed facts from the last 8 messages and stores them. This is the primary memory capture mechanism.

**Every 5 turns**, a session summary is additionally auto-saved.

**Per-turn semantic refresh**: On turns 2+, the "Relevant to Current Task" section is re-searched against the current user message, so context stays aligned with what the user is asking about right now.

**Compaction survival**: When context is compacted (at ~80% usage), the plugin injects memories into the compaction summary, then triggers a full [MEMORY] block rebuild on the next turn. Memory is never lost to compaction.

**Privacy**: Content wrapped in `<private>...</private>` tags is stripped before extraction. Nothing inside private tags leaves the machine.

**Memory aging rules** (handled automatically by the server):
- `progress`: only the latest survives ‚Äî older ones are deleted
- `session-summary`: capped at 3 per project; oldest are condensed into a `learned-pattern` then deleted
- Structural types (`architecture`, `tech-context`, etc.): evolved understanding updates the existing memory rather than duplicating it

## Your role

- **Read and use the `[MEMORY]` block** ‚Äî it is your project continuity. Treat it as ground truth for the current project state.
- **Never ask the user** to "save", "load", or manage memory ‚Äî it is fully invisible and automatic.
- **Never announce** that you are saving memory or that memory was loaded.
- **When the user explicitly asks you to remember something** (e.g. "remember this", "save this for later"), use the `memory` tool with `mode: "add"` immediately. Do not skip this.

## Memory tool

You have access to a `memory` tool. Use it when:
- The user explicitly asks you to remember something
- You discover something genuinely important mid-session that won't be captured by auto-save (e.g. a key architectural decision, a tricky bug fix, a strong user preference)
- You need context not in the `[MEMORY]` block ‚Äî search proactively when you detect a task switch, encounter unfamiliar references, or need historical context

**Scopes:**
- `scope: "project"` ‚Äî for project-specific knowledge (default)
- `scope: "user"` ‚Äî for cross-project preferences (e.g. "prefers concise responses", "always uses bun over npm")

**Types to use:**
| Type | When to use |
|------|------------|
| `project-brief` | What the project is, its purpose |
| `architecture` | System design, component relationships |
| `tech-context` | Stack, tools, languages, patterns |
| `product-context` | Features, goals, product decisions |
| `progress` | Current state, what's done, what's blocked |
| `project-config` | Config preferences, run commands, env setup |
| `error-solution` | Bug fixes, workarounds discovered |
| `preference` | User coding preferences |
| `learned-pattern` | Patterns observed across the project |
| `session-summary` | Auto-generated ‚Äî do not use manually |
| `conversation` | Raw conversation context ‚Äî rarely needed manually |

**Example explicit save:**
```
memory({ mode: "add", content: "User prefers bun over npm for all installs", scope: "user", type: "preference" })
memory({ mode: "add", content: "Auth uses JWT stored in httpOnly cookies, not localStorage", scope: "project", type: "architecture" })
```

**Search when needed:**
```
memory({ mode: "search", query: "how is authentication handled" })
memory({ mode: "list", scope: "project", limit: 10 })
memory({ mode: "profile" })
memory({ mode: "forget", memoryId: "abc-123" })
```
````

</details>

> **Already have an `AGENTS.md`?** This file is global ‚Äî OpenCode injects it into every session. If you already have one with other instructions (security rules, coding preferences, etc.), append the Memory System section to it rather than replacing it.

---

### Step 6 ‚Äî Start a session

Open any project in OpenCode. On your first message, you'll see a `[MEMORY]` block injected into context ‚Äî this confirms the plugin is running. Memories will auto-save after every assistant turn from here on.

**That's it.** The agent now remembers across sessions automatically.

---

## How It Works

### Every LLM call ‚Äî system prompt injection

The `[MEMORY]` block lives in the **system prompt**, not in message history. It is rebuilt fresh on every LLM call via the `experimental.chat.system.transform` hook:

- **Turn 1**: 4 parallel API calls fetch profile, user memories, project memories, and semantic search results. All cached in a per-session `SessionMemoryCache`.
- **Turns 2+**: A single semantic search call (~300ms) refreshes the "Relevant to Current Task" section with results matching the current message topic.
- **Every LLM call** (including tool continuations): `system.transform` reads the cache, rebuilds the `[MEMORY]` block, and pushes it into the system prompt.

This means:
- **Zero token accumulation** ‚Äî the system prompt is rebuilt each call, not appended to history
- **Survives compaction** ‚Äî the system prompt is never summarized away
- **Always fresh** ‚Äî topic switches within a session cause different memories to surface

The agent sees this in its system prompt:

```
[MEMORY]

## Project Brief
- opencode-memory is a self-hosted persistent memory system for OpenCode AI agents.
- Goal: automatic, silent memory across sessions ‚Äî no user commands required.

## Tech Context
- Plugin lives at plugin/ subdirectory; opencode.json must point there not repo root
- Docker Compose uses env_file to load .env ‚Äî do not export keys in shell
- Backend uses voyage-code-3 embeddings; extraction via claude-haiku-4-5 (default)

## Architecture
- Backend refactored from monolithic main.py into app/ package with focused modules
- Plugin ‚Üí HTTP:8020 ‚Üí Memory Server ‚Üí LanceDB; Dashboard ‚Üí HTTP:8020 ‚Üí Memory Server

## Progress & Status
- Memory server running at localhost:8020, dashboard at localhost:3030
- Benchmark at 93.5% avg (3 runs, 92.0‚Äì95.0%); cross-synthesis at ~71% is primary remaining gap

## Last Session
- Improved extraction prompt to preserve causal chains for error-solution memories
- Added architecture to synthesis retrieval types; cross-synthesis 76% ‚Üí 80%

## User Preferences
- Use bun not npm for all plugin builds and installs
- Prefers concise responses; no emojis unless explicitly requested

## Relevant to Current Task
- [94%, 2026-02-23] Default extraction model is claude-haiku-4-5 ‚Äî 93.5% avg benchmark, most consistent, ~$0.11/session
- [88%, 2026-02-21] Contradiction detection marks superseded memories with superseded_by field; excluded from retrieval
```

### After every turn ‚Äî auto-save

Every time the assistant completes a turn, the plugin automatically:

1. Snapshots the recent conversation (last 8 real exchanges)
2. Sends them to `POST /memories`
3. The configured LLM extracts a JSON array of typed, memorable facts
4. Each fact is embedded with `voyage-code-3` and stored in LanceDB after a cosine dedup check
5. The raw source conversation text is stored alongside each memory as a `chunk` (enables hybrid search)
6. A contradiction search (cosine distance ‚â§ 0.5) finds semantically related existing memories; an LLM call identifies any that the new memory supersedes ‚Äî those are marked `superseded_by` and excluded from future retrieval

A 15-second cooldown handles OpenCode's double-fire of the completion event.

The extraction prompt instructs the model to capture:
- Mistakes made and then corrected, including *why* they were wrong
- Decisions and the reasoning behind them ‚Äî not just "used X" but "used X because Y"
- User preferences revealed through corrections or explicit requests
- Approaches that **failed** and why ‚Äî so they won't be repeated

### Memory types

| Type | What it captures |
|------|-----------------|
| `project-brief` | Core project definition, goals, scope |
| `architecture` | System design, patterns, component relationships |
| `tech-context` | Stack, tools, build commands, constraints |
| `product-context` | Why the project exists, problems solved |
| `progress` | Current state ‚Äî only the latest entry survives |
| `session-summary` | What was worked on; oldest condense into `learned-pattern` |
| `error-solution` | Bug fixes, gotchas, approaches that failed |
| `preference` | Cross-project personal preferences, workflow habits |
| `learned-pattern` | Reusable patterns condensed from past sessions |

### Explicit saves

Say "remember this", "don't forget", or "memorize" and the agent immediately stores the current context using the `memory` tool ‚Äî independently of the auto-save cycle.

### Compaction survival

When the context window approaches capacity, OpenCode summarises the conversation and removes old messages. Because the `[MEMORY]` block lives in the system prompt (not in message history), it is never affected by compaction ‚Äî the agent retains full project context even after truncation. The plugin also intercepts the compaction hook to inject memories into the compaction context for richer summaries.

---

## Extraction Providers

Memory extraction is a well-defined, deterministic task: read a conversation, output a JSON array of typed facts. It doesn't need deep reasoning ‚Äî it needs to be reliable, structured, and consistent. The backend supports three providers, selectable via a single env var:

| Provider | Model | Speed | Cost (in/out per MTok) | Benchmark | Notes |
|---|---|---|---|---|---|
| **Anthropic** (default) | `claude-haiku-4-5` | ~14s/session | $1.00 / $5.00 | **93.5% avg** (92.0‚Äì95.0%, 3pp variance) | Most consistent ‚Äî recommended |
| **xAI** | `grok-4-1-fast-non-reasoning` | ~5s/session | $0.20 / $0.50 | ~86.5% avg (78.5‚Äì94.5%, 16pp variance) | Fastest and cheapest; high run-to-run variance |
| **Google** | `gemini-3-flash-preview` | ~21s/session | $0.50 / $3.00 | ‚Äî | Native JSON mode; high TTFT latency |

```bash
# Switch provider ‚Äî edit .env, then restart
EXTRACTION_PROVIDER=xai   # or "anthropic" (default) or "google"
docker compose up -d
```

**Why Haiku is the default:** Across 3 benchmark runs (200 questions each), Haiku averaged 93.5% accuracy with only 3pp variance (92.0‚Äì95.0%), while Grok averaged ~86.5% with 16pp variance (78.5‚Äì94.5%). The cost difference is negligible in practice ‚Äî a 20-turn session costs ~$0.11 with Haiku vs ~$0.01 with Grok, both dwarfed by the main model cost (Claude Opus/Sonnet at $15‚Äì$75/MTok). Extraction runs in the background after each assistant turn, so the ~14s latency is invisible to users ‚Äî retrieval uses Voyage AI embeddings, not the extraction provider. Consistency wins over speed for a memory system where every extracted fact persists across sessions.

> **Why not a reasoning model?** During testing, a frontier reasoning model consumed all `max_completion_tokens` on internal chain-of-thought and returned empty output with `finish_reason: "length"` every single time. Reasoning models are the wrong tool for deterministic structured extraction.

### Voyage AI `voyage-code-3` ‚Äî embeddings

`voyage-code-3` is specifically trained on code and technical content. It produces 1024-dimensional embeddings optimised for semantic similarity in programming contexts ‚Äî code snippets, error messages, variable names, and technical prose all embed far more meaningfully than with a general-purpose model like `text-embedding-3-large`.

This matters because most memories *are* about technical decisions and code patterns. Better embeddings mean better retrieval ‚Äî which means more relevant context injected at session start.

Pricing: **$0.18/M tokens** with a generous free tier.

---

## Benchmark

Memory quality is measured by [DevMemBench](./benchmark/README.md) ‚Äî a coding-assistant-specific benchmark built for this project. It ingests 25 synthetic sessions from a realistic FastAPI/PostgreSQL/Redis/Stripe + Next.js project, then evaluates retrieval and answer quality across 200 questions in 8 categories using an LLM-as-judge pipeline.

> Model: `claude-sonnet-4-6` (judge + answerer) ¬∑ 200 questions ¬∑ 25 sessions ¬∑ K=20 retrieval

```
tech-stack        ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 100%  (25/25)  ‚úì
preference        ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 100%  (25/25)  ‚úì
architecture      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 100%  (25/25)  ‚úì
error-solution    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 100%  (25/25)  ‚úì
session-cont.     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë  92%  (23/25)  ‚úì
knowledge-update  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë  92%  (23/25)  ‚úì
abstention        ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë  92%  (23/25)  ‚úì
cross-synthesis   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë  80%  (20/25)  ‚ö†  primary remaining gap
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Overall           94.5%  (189/200)
```

Each technique improvement is tracked against the benchmark:

| Technique | Benchmark impact |
|---|---|
| Hybrid search ‚Äî source chunk storage | error-solution 0% ‚Üí 100%, +32.5pp overall |
| Relational versioning ‚Äî supersede stale memories | knowledge-update 40% ‚Üí 100% |
| Temporal grounding ‚Äî recency boost + date in context | session-continuity 20% ‚Üí 60% |
| Natural question phrasing (v2-natural) | session-continuity 24% ‚Üí 88% (+64pp) |
| K=20 retrieval (k20-synthesis-fix) | cross-synthesis 52% ‚Üí 64% (+12pp), overall 88% ‚Üí 91% |
| Hybrid enumeration routing | cross-synthesis 64% ‚Üí 76% (+12pp), overall 91% ‚Üí 92% |
| Causal-chain extraction + architecture synthesis | architecture/error-solution ‚Üí 100%, overall 92% ‚Üí 94.5% |

See [`benchmark/README.md`](./benchmark/README.md) for full results history and how to run it yourself.

---

## Architecture

```
opencode-memory/
‚îú‚îÄ‚îÄ plugin/       TypeScript ¬∑ Bun ‚Äî OpenCode plugin (hooks + memory tool)
‚îú‚îÄ‚îÄ backend/      Python 3.13 ¬∑ FastAPI ¬∑ LanceDB ‚Äî memory API server
‚îÇ   ‚îî‚îÄ‚îÄ app/
‚îÇ       ‚îú‚îÄ‚îÄ main.py        app, lifespan, router wiring
‚îÇ       ‚îú‚îÄ‚îÄ config.py      env vars, model names, pricing
‚îÇ       ‚îú‚îÄ‚îÄ models.py      Pydantic models + LanceDB schema
‚îÇ       ‚îú‚îÄ‚îÄ prompts.py     LLM extraction prompt templates
‚îÇ       ‚îú‚îÄ‚îÄ telemetry.py   CostLedger + ActivityLog
‚îÇ       ‚îú‚îÄ‚îÄ registry.py    NameRegistry (hash ‚Üí display name)
‚îÇ       ‚îú‚îÄ‚îÄ db.py          LanceDB table state
‚îÇ       ‚îú‚îÄ‚îÄ embedder.py    Voyage AI wrapper
‚îÇ       ‚îú‚îÄ‚îÄ extractor.py   multi-provider LLM dispatch + extraction logic
‚îÇ       ‚îú‚îÄ‚îÄ store.py       dedup, aging rules, CRUD helpers
‚îÇ       ‚îî‚îÄ‚îÄ routes/        memories ¬∑ projects ¬∑ system
‚îú‚îÄ‚îÄ frontend/     Next.js 15 ¬∑ Tailwind ‚Äî web dashboard
‚îî‚îÄ‚îÄ scripts/      start.sh ‚Äî docker compose orchestration helper
```

```mermaid
flowchart TD
    subgraph PLUGIN["üîå OpenCode Plugin ‚Äî Bun ¬∑ TypeScript"]
        H1["chat.message ‚Äî turn 1: populates session cache; turns 2+: semantic refresh"]
        H5["system.transform ‚Äî rebuilds and injects MEMORY into system prompt every LLM call"]
        H2["messages.transform ‚Äî caches full conversation before every LLM call"]
        H3["event: message.updated ‚Äî auto-saves atomic facts after every completed turn"]
        H4["compaction hook ‚Äî preserves memories through context window truncation"]
    end

    PLUGIN -->|"HTTP ¬∑ localhost:8020"| SERVER

    subgraph SERVER["‚öôÔ∏è Memory Server ‚Äî Python ¬∑ FastAPI ¬∑ Docker"]
        subgraph WRITE["POST /memories"]
            direction LR
            W1["LLM extraction<br/>(xAI ¬∑ Anthropic ¬∑ Google)<br/>extract typed facts + chunks"] --> W2["Voyage voyage-code-3<br/>embed ¬∑ 1024 dims"] --> W3["LanceDB<br/>cosine dedup ‚Üí upsert"] --> W4["LLM contradiction<br/>detect ‚Üí supersede stale"]
        end
        subgraph FIND["POST /memories/search"]
            direction LR
            F1["Voyage voyage-code-3<br/>embed query"] --> F2["LanceDB<br/>ANN cosine ¬∑ top-k<br/>filter superseded_by = ''"] --> F3["recency blend<br/>(optional weight)"]
        end
        REST["GET /memories ¬∑ /projects ¬∑ /stats ¬∑ /costs ¬∑ /health ¬∑ /activity"]
        VOL[("opencode-memory-data ‚Äî Docker volume ¬∑ Apache Arrow ¬∑ Parquet")]
        WRITE --> VOL
        FIND --> VOL
    end

    DASH -->|"HTTP ¬∑ localhost:8020"| SERVER

    subgraph DASH["üñ•Ô∏è Dashboard ‚Äî Next.js 15 ¬∑ Tailwind ¬∑ localhost:3030"]
        UI["Memory browser ¬∑ Cost tracker ¬∑ Live activity feed"]
    end

    classDef hook fill:#0f172a,stroke:#38bdf8,color:#bae6fd
    classDef ai   fill:#052e16,stroke:#4ade80,color:#bbf7d0
    classDef rest fill:#1c1917,stroke:#fb923c,color:#fed7aa
    classDef db   fill:#1e1b4b,stroke:#a78bfa,color:#ddd6fe
    classDef ui   fill:#2d1b4e,stroke:#c084fc,color:#f3e8ff

    class H1,H2,H3,H4,H5 hook
    class W1,W2,W3,F1,F2 ai
    class REST rest
    class VOL db
    class UI ui
```

---

## Configuration

The plugin reads an optional config file at `~/.config/opencode/memory.jsonc`:

```jsonc
{
  // Memory server URL (default: http://localhost:8020)
  "memoryBaseUrl": "http://localhost:8020",

  // Minimum similarity score for retrieval, 0‚Äì1 (default: 0.45)
  // Higher values cut noise; lower values improve recall on broad queries
  "similarityThreshold": 0.45,

  // Max memories retrieved per scope per session (default: 10)
  // Applied separately to user-scope and project-scope searches, so
  // total semantic memories available = up to 2√ó this value.
  // Benchmark runs at K=20 (project+user combined) ‚Äî this matches that.
  "maxMemories": 10,

  // Context fill ratio that triggers compaction hook (default: 0.80)
  "compactionThreshold": 0.80,

  // Additional keyword patterns that trigger an explicit save (regex strings)
  // "keywordPatterns": ["bookmark this", "save for later"]
}
```

---

## Development

```bash
# Rebuild the TypeScript plugin after making changes
cd plugin
bun run build

# Type-check only (no output)
bun run typecheck
```

```bash
# Rebuild and restart the memory server after backend changes
docker compose build opencode-memory-backend
docker compose up -d opencode-memory-backend
```

**Useful commands:**

```bash
# Tail plugin logs
tail -f ~/.opencode-memory.log

# Tail memory server logs
docker compose logs -f opencode-memory-backend

# Check server health
curl http://localhost:8020/health

# List all projects with memory counts
curl http://localhost:8020/projects

# Check accumulated API costs
curl http://localhost:8020/costs

# Wipe all memories (destructive ‚Äî stops containers and deletes volume)
docker compose down -v
```

---

## Key Engineering Notes

Non-obvious findings from building on the OpenCode plugin API:

1. **`finish` is a string, not a boolean.** Values are `"tool-calls"` (mid-turn) and `"stop"` (turn complete). Checking `finish === true` never matches.

2. **`session.idle` doesn't fire reliably.** Use `message.updated` with `finish === "stop"` for post-turn processing instead.

3. **`ctx.client.session.messages()` always returns 401.** The plugin's HTTP client isn't pre-configured with the OpenCode auth token. Use `experimental.chat.messages.transform` to read conversation messages.

4. **`finish: "stop"` fires twice per turn.** A 15-second cooldown deduplicates it.

5. **Three plugin instances initialise per session.** OpenCode creates one per open directory. They share module-level state via Bun's module system ‚Äî singletons are safe.

6. **Docker Compose shell env vars override `.env`.** If `XAI_API_KEY` is set in your shell, it wins over the file. Use `env_file: - .env` to always load from the file directly.

---

## Stopping and Resetting

```bash
# Stop the server, keep all memories
docker compose down

# Stop and permanently wipe all memories
docker compose down -v
```

---

## Privacy

All data stays on your machine. The only outbound API calls are:

- **Voyage AI** ‚Äî text is sent to generate embeddings. Voyage does not store your data.
- **Your extraction provider** (xAI, Anthropic, or Google) ‚Äî conversation text is sent for memory extraction. Only one provider is called per request, determined by `EXTRACTION_PROVIDER`.

To exclude sensitive content from extraction, wrap it in `<private>...</private>` ‚Äî it will be stripped before any text leaves your machine.

---

## License

[MIT](./LICENSE) ‚Äî use it, fork it, ship it.

---

<div align="center">

Built with [OpenCode](https://opencode.ai) ¬∑ [LanceDB](https://lancedb.com) ¬∑ [Voyage AI](https://www.voyageai.com) ¬∑ [xAI](https://x.ai) ¬∑ [Anthropic](https://anthropic.com) ¬∑ [Google AI](https://ai.google.dev) ¬∑ [FastAPI](https://fastapi.tiangolo.com) ¬∑ [Bun](https://bun.sh)

</div>
