<div align="center">

# opencode-memory

**Fully self-hosted, autonomous persistent memory for [OpenCode](https://opencode.ai) AI agents.**

The agent learns from every session automatically ‚Äî no commands, no manual saves, no cloud.

<br/>

[![License: MIT](https://img.shields.io/badge/License-MIT-F7DF1E?style=flat)](https://opensource.org/licenses/MIT)
[![TypeScript](https://img.shields.io/badge/TypeScript-5.7-3178C6?style=flat&logo=typescript&logoColor=white)](https://www.typescriptlang.org/)
[![Python](https://img.shields.io/badge/Python-3.13-3776AB?style=flat&logo=python&logoColor=white)](https://www.python.org/)
[![Bun](https://img.shields.io/badge/Bun-Runtime-FBF0DF?style=flat&logo=bun&logoColor=black)](https://bun.sh/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.115-009688?style=flat&logo=fastapi&logoColor=white)](https://fastapi.tiangolo.com/)
[![Next.js](https://img.shields.io/badge/Next.js-15-000000?style=flat&logo=nextdotjs&logoColor=white)](https://nextjs.org/)
[![Docker](https://img.shields.io/badge/Docker-Compose-2496ED?style=flat&logo=docker&logoColor=white)](https://www.docker.com/)
[![LanceDB](https://img.shields.io/badge/LanceDB-Vector_DB-CF3CFF?style=flat)](https://lancedb.com/)
[![xAI Grok](https://img.shields.io/badge/xAI-Grok-000000?style=flat&logo=x&logoColor=white)](https://x.ai/)
[![Voyage AI](https://img.shields.io/badge/Voyage_AI-Code_Embeddings-5B6BF5?style=flat)](https://www.voyageai.com/)
[![Self-Hosted](https://img.shields.io/badge/Self--Hosted-100%25_Local-22C55E?style=flat&logo=homeassistant&logoColor=white)](https://github.com/prosperitypirate/opencode-memory)
[![OpenCode Plugin](https://img.shields.io/badge/OpenCode-Plugin-FF6B35?style=flat)](https://opencode.ai)

</div>

<div align="center">

![opencode-memory dashboard](.github/assets/dashboard.png)

</div>

---

## What is this?

OpenCode starts every session from scratch. No memory of past decisions, established patterns, previous mistakes, or project context. Every session, you repeat yourself.

**opencode-memory fixes this permanently.**

After every assistant turn, the conversation is automatically analysed. Key facts are extracted and stored locally on your machine. At the start of every new session, relevant memories are silently injected into the agent's context. From the agent's perspective, it simply *remembers* ‚Äî across days, weeks, and projects.

**The agent never thinks about saving memories. It just happens.**

---

## Vision

Memory should be infrastructure, not a ritual.

Today, every AI coding session starts from zero. Tools like Cline's Memory Bank made this disciplined ‚Äî but still require `/load` at the start of each session and `/save` at the end. The agent can't remember without your help. You are the memory system.

**This is the wrong model.** Memory should operate entirely beneath the agent ‚Äî invisible, automatic, never requiring a command.

OpenCode makes something remarkable possible: a [plugin hook architecture](https://opencode.ai/docs) that fires on every session start, every assistant turn, and every compaction event. These hooks are the foundation. They allow memory to be captured and injected with complete transparency ‚Äî the agent receives context it never explicitly asked for, and the user issues no commands they didn't intend.

The result is a new category: **infrastructure-level memory.**

The broader vision goes further:

- **Any agent, any tool** ‚Äî the memory backend is a standalone local API. Any editor plugin, any agent framework, can query it. The OpenCode plugin is one implementation. VS Code, Cursor, Zed, and others can follow the same pattern.
- **Zero user overhead** ‚Äî memory is captured automatically via hooks after every turn. The user never issues a command, never updates a file, never thinks about memory management.
- **Private by default** ‚Äî everything lives on your machine. No cloud sync, no accounts, no telemetry.
- **Self-improving** ‚Äî memories deduplicate, contradict, condense, and age automatically. The corpus becomes more accurate over time, not more bloated.

The goal is not a better memory bank.

**It is the end of the memory bank as a concept** ‚Äî replaced by invisible memory infrastructure that any agent, in any tool, simply inherits when it starts work.

---

## Features

- **Fully automatic** ‚Äî memories save after every assistant turn with zero user action
- **100% local** ‚Äî LanceDB runs embedded in-process; all data lives in a Docker volume on your machine
- **Always-fresh context** ‚Äî the `[MEMORY]` block is injected into the system prompt on every LLM call via `system.transform` hook, not as a one-time message part ‚Äî zero token accumulation, rebuilt fresh each turn
- **Per-turn semantic refresh** ‚Äî on every user message, a semantic search updates the "Relevant to Current Task" section to match the current conversation topic ‚Äî the agent always sees context aligned to what you're asking about right now
- **Compaction-proof** ‚Äî memory lives in the system prompt, which is never compacted ‚Äî long sessions never lose project context, no matter how many turns
- **Code-optimised embeddings** ‚Äî Voyage `voyage-code-3` is purpose-built for code and technical content
- **Typed memory system** ‚Äî memories are classified (`architecture`, `error-solution`, `preference`, `progress`, etc.) and injected in structured blocks
- **Hybrid search** ‚Äî semantic search on atomic memory facts for retrieval, raw source chunk injected into context for exact values (config numbers, error strings, function names)
- **Smart deduplication** ‚Äî cosine similarity check prevents duplicate memories from accumulating
- **Relational versioning** ‚Äî when new memories contradict existing ones, stale entries are automatically marked superseded and excluded from future retrieval
- **Memory aging** ‚Äî session summaries auto-condense into compact learned patterns over time; only the latest `progress` entry survives
- **Temporal grounding** ‚Äî search results carry their session date; recency blending can optionally boost recent memories for time-sensitive queries
- **Project + user scope** ‚Äî separate namespaces for project-specific knowledge vs. cross-project personal preferences
- **Explicit save support** ‚Äî say "remember this" and the agent immediately stores it
- **Web dashboard** ‚Äî live activity feed, cost tracking, per-project memory browser at `http://localhost:3030`
- **Privacy filter** ‚Äî wrap content in `<private>...</private>` tags to exclude it from extraction

---

## How it compares

| | **opencode-memory** | Cline memory bank | [Supermemory](https://supermemory.ai) plugin | [mem0](https://mem0.ai) fork |
|---|---|---|---|---|
| **Storage** | Local (LanceDB embedded) | Markdown files in repo | Supermemory cloud | Local (Qdrant) |
| **Embeddings** | Voyage `voyage-code-3` | None | Cloud | OpenAI `text-embedding-3-large` |
| **Extraction** | xAI Grok (automatic) | LLM via `/save` (manual) | Cloud | OpenAI `gpt-4o` |
| **User action required** | ‚ùå Zero | ‚úÖ `/load` + `/save` each session | ‚ùå Zero | ‚ùå Zero |
| **Data privacy** | ‚úÖ 100% local | ‚úÖ 100% local | ‚ùå Cloud | ‚úÖ Local |
| **Code-optimised embeddings** | ‚úÖ | ‚ùå | ‚ùå | ‚ùå |
| **Fully automatic saves** | ‚úÖ Every turn | ‚ùå Manual | ‚ùå Keyword-only | ‚ùå Keyword-only |
| **Semantic retrieval** | ‚úÖ | ‚ùå Full file dump | ‚úÖ | ‚úÖ |
| **Session token overhead** | ~3K tokens (system prompt, zero accumulation) | Up to 50K tokens | Varies | Varies |
| **Survives compaction** | ‚úÖ System prompt injection | ‚ùå | ‚ùå | ‚ùå |
| **Per-turn refresh** | ‚úÖ Semantic refresh every turn | ‚ùå | ‚ùå | ‚ùå |

---

## Quick Start

### Prerequisites

| Tool | Purpose | Install |
|------|---------|---------|
| [Docker Desktop](https://www.docker.com/products/docker-desktop/) | Runs the memory server and dashboard | [docker.com](https://www.docker.com/products/docker-desktop/) |
| [Bun](https://bun.sh) | Builds the OpenCode plugin | `curl -fsSL https://bun.sh/install \| bash` |
| [OpenCode](https://opencode.ai) | The AI coding agent this extends | `npm i -g opencode-ai` |
| xAI API key | Memory extraction via Grok | [console.x.ai](https://console.x.ai) |
| Voyage AI API key | Code embeddings | [voyageai.com](https://www.voyageai.com) ‚Äî free tier available |

---

### Step 1 ‚Äî Clone and build the plugin

```bash
git clone https://github.com/prosperitypirate/opencode-memory
cd opencode-memory/plugin
bun install
bun run build
```

This compiles the TypeScript OpenCode plugin into `plugin/dist/`.

---

### Step 2 ‚Äî Add your API keys

```bash
cd ..                    # back to repo root
cp .env.example .env
```

Open `.env` and fill in your keys:

```env
# xAI API key ‚Äî used for memory extraction (grok-4-1-fast-non-reasoning)
# Get yours at: https://console.x.ai
XAI_API_KEY=xai-...

# Voyage AI API key ‚Äî used for code embeddings (voyage-code-3)
# Get yours at: https://www.voyageai.com  (free tier available)
VOYAGE_API_KEY=pa-...
```

> **Note:** The Docker container reads keys directly from `.env` via `env_file`. Do not export these in your shell ‚Äî a shell environment variable will override the file and the server won't pick up your key.

---

### Step 3 ‚Äî Start the memory server and dashboard

```bash
docker compose up -d
```

This starts two containers:

| Container | Port | Purpose |
|-----------|------|---------|
| `opencode-memory-backend` | `8020` | FastAPI + LanceDB memory server |
| `opencode-memory-frontend` | `3030` | Next.js web dashboard |

Verify the backend is ready:

```bash
curl http://localhost:8020/health
# {"status":"ok","ready":true}
```

Open the dashboard: **[http://localhost:3030](http://localhost:3030)**

Data persists in the `opencode-memory-data` Docker named volume across restarts.

---

### Step 4 ‚Äî Register the plugin with OpenCode

Run the installer from the `plugin/` directory:

```bash
cd plugin
node dist/cli.js install
```

The installer automatically:
- Adds the plugin to `~/.config/opencode/opencode.json`
- Creates a `/memory-init` slash command

> **Manual alternative:** If you prefer, edit `~/.config/opencode/opencode.json` directly and add the absolute path to the `plugin/` subdirectory:
> ```json
> {
>   "$schema": "https://opencode.ai/config.json",
>   "plugin": [
>     "file:///absolute/path/to/opencode-memory/plugin"
>   ]
> }
> ```

---

### Step 5 ‚Äî Start a session

Open any project in OpenCode. On your first message, you'll see a `[MEMORY]` block injected into context ‚Äî this confirms the plugin is running. Memories will auto-save after every assistant turn from here on.

**That's it.** The agent now remembers across sessions automatically.

---

## How It Works

### Every LLM call ‚Äî system prompt injection

The `[MEMORY]` block lives in the **system prompt**, not in message history. It is rebuilt fresh on every LLM call via the `experimental.chat.system.transform` hook:

- **Turn 1**: 4 parallel API calls fetch profile, user memories, project memories, and semantic search results. All cached in a per-session `SessionMemoryCache`.
- **Turns 2+**: A single semantic search call (~300ms) refreshes the "Relevant to Current Task" section with results matching the current message topic.
- **Every LLM call** (including tool continuations): `system.transform` reads the cache, rebuilds the `[MEMORY]` block, and pushes it into the system prompt.

This means:
- **Zero token accumulation** ‚Äî the system prompt is rebuilt each call, not appended to history
- **Survives compaction** ‚Äî the system prompt is never summarized away
- **Always fresh** ‚Äî topic switches within a session cause different memories to surface

The agent sees this in its system prompt:

```
[MEMORY]

## Project Brief
- opencode-memory is a self-hosted persistent memory system for OpenCode AI agents.
- Goal: automatic, silent memory across sessions ‚Äî no user commands required.

## Tech Context
- Plugin lives at plugin/ subdirectory; opencode.json must point there not repo root
- Docker Compose uses env_file to load .env ‚Äî do not export keys in shell
- Backend uses voyage-code-3 embeddings; extraction via grok-4-1-fast-non-reasoning

## Architecture
- Backend refactored from monolithic main.py into app/ package with focused modules
- Plugin ‚Üí HTTP:8020 ‚Üí Memory Server ‚Üí LanceDB; Dashboard ‚Üí HTTP:8020 ‚Üí Memory Server

## Progress & Status
- Memory server running at localhost:8020, dashboard at localhost:3030
- Benchmark at 94.5% (189/200); cross-synthesis at 80% is primary remaining gap

## Last Session
- Improved extraction prompt to preserve causal chains for error-solution memories
- Added architecture to synthesis retrieval types; cross-synthesis 76% ‚Üí 80%

## User Preferences
- Use bun not npm for all plugin builds and installs
- Prefers concise responses; no emojis unless explicitly requested

## Relevant to Current Task
- [94%, 2026-02-21] Backend extraction model is grok-4-1-fast-non-reasoning ‚Äî fast, structured JSON, fractions of a cent per session
- [88%, 2026-02-21] Contradiction detection marks superseded memories with superseded_by field; excluded from retrieval
```

### After every turn ‚Äî auto-save

Every time the assistant completes a turn, the plugin automatically:

1. Snapshots the recent conversation (last 8 real exchanges)
2. Sends them to `POST /memories`
3. Grok extracts a JSON array of typed, memorable facts
4. Each fact is embedded with `voyage-code-3` and stored in LanceDB after a cosine dedup check
5. The raw source conversation text is stored alongside each memory as a `chunk` (enables hybrid search)
6. A contradiction search (cosine distance ‚â§ 0.5) finds semantically related existing memories; an LLM call identifies any that the new memory supersedes ‚Äî those are marked `superseded_by` and excluded from future retrieval

A 15-second cooldown handles OpenCode's double-fire of the completion event.

The extraction prompt instructs the model to capture:
- Mistakes made and then corrected, including *why* they were wrong
- Decisions and the reasoning behind them ‚Äî not just "used X" but "used X because Y"
- User preferences revealed through corrections or explicit requests
- Approaches that **failed** and why ‚Äî so they won't be repeated

### Memory types

| Type | What it captures |
|------|-----------------|
| `project-brief` | Core project definition, goals, scope |
| `architecture` | System design, patterns, component relationships |
| `tech-context` | Stack, tools, build commands, constraints |
| `product-context` | Why the project exists, problems solved |
| `progress` | Current state ‚Äî only the latest entry survives |
| `session-summary` | What was worked on; oldest condense into `learned-pattern` |
| `error-solution` | Bug fixes, gotchas, approaches that failed |
| `preference` | Cross-project personal preferences, workflow habits |
| `learned-pattern` | Reusable patterns condensed from past sessions |

### Explicit saves

Say "remember this", "don't forget", or "memorize" and the agent immediately stores the current context using the `memory` tool ‚Äî independently of the auto-save cycle.

### Compaction survival

When the context window approaches capacity, OpenCode summarises the conversation and removes old messages. Because the `[MEMORY]` block lives in the system prompt (not in message history), it is never affected by compaction ‚Äî the agent retains full project context even after truncation. The plugin also intercepts the compaction hook to inject memories into the compaction context for richer summaries.

---

## Why xAI + Voyage AI?

### xAI `grok-4-1-fast-non-reasoning` ‚Äî memory extraction

Memory extraction is a well-defined, deterministic task: read a conversation, output a JSON array. It doesn't need deep reasoning ‚Äî it needs to be fast, cheap, and reliably structured.

`grok-4-1-fast-non-reasoning` is precisely that. It returns structured JSON immediately without burning tokens on internal chain-of-thought. Cost per session: **fractions of a cent** ($0.20/M input ¬∑ $0.50/M output).

> **Why not a reasoning model?** During testing, a frontier reasoning model consumed all `max_completion_tokens` on internal chain-of-thought and returned empty output with `finish_reason: "length"` every single time. Reasoning models are the wrong tool for deterministic structured extraction.

### Voyage AI `voyage-code-3` ‚Äî embeddings

`voyage-code-3` is specifically trained on code and technical content. It produces 1024-dimensional embeddings optimised for semantic similarity in programming contexts ‚Äî code snippets, error messages, variable names, and technical prose all embed far more meaningfully than with a general-purpose model like `text-embedding-3-large`.

This matters because most memories *are* about technical decisions and code patterns. Better embeddings mean better retrieval ‚Äî which means more relevant context injected at session start.

Pricing: **$0.18/M tokens** with a generous free tier.

---

## Benchmark

Memory quality is measured by [DevMemBench](./benchmark/README.md) ‚Äî a coding-assistant-specific benchmark built for this project. It ingests 25 synthetic sessions from a realistic FastAPI/PostgreSQL/Redis/Stripe + Next.js project, then evaluates retrieval and answer quality across 200 questions in 8 categories using an LLM-as-judge pipeline.

> Model: `claude-sonnet-4-6` (judge + answerer) ¬∑ 200 questions ¬∑ 25 sessions ¬∑ K=20 retrieval

```
tech-stack        ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 100%  (25/25)  ‚úì
preference        ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 100%  (25/25)  ‚úì
architecture      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 100%  (25/25)  ‚úì
error-solution    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 100%  (25/25)  ‚úì
session-cont.     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë  92%  (23/25)  ‚úì
knowledge-update  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë  92%  (23/25)  ‚úì
abstention        ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë  92%  (23/25)  ‚úì
cross-synthesis   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë  80%  (20/25)  ‚ö†  primary remaining gap
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Overall           94.5%  (189/200)
```

Each technique improvement is tracked against the benchmark:

| Technique | Benchmark impact |
|---|---|
| Hybrid search ‚Äî source chunk storage | error-solution 0% ‚Üí 100%, +32.5pp overall |
| Relational versioning ‚Äî supersede stale memories | knowledge-update 40% ‚Üí 100% |
| Temporal grounding ‚Äî recency boost + date in context | session-continuity 20% ‚Üí 60% |
| Natural question phrasing (v2-natural) | session-continuity 24% ‚Üí 88% (+64pp) |
| K=20 retrieval (k20-synthesis-fix) | cross-synthesis 52% ‚Üí 64% (+12pp), overall 88% ‚Üí 91% |
| Hybrid enumeration routing | cross-synthesis 64% ‚Üí 76% (+12pp), overall 91% ‚Üí 92% |
| Causal-chain extraction + architecture synthesis | architecture/error-solution ‚Üí 100%, overall 92% ‚Üí 94.5% |

See [`benchmark/README.md`](./benchmark/README.md) for full results history and how to run it yourself.

---

## Architecture

```
opencode-memory/
‚îú‚îÄ‚îÄ plugin/       TypeScript ¬∑ Bun ‚Äî OpenCode plugin (hooks + memory tool)
‚îú‚îÄ‚îÄ backend/      Python 3.13 ¬∑ FastAPI ¬∑ LanceDB ‚Äî memory API server
‚îÇ   ‚îî‚îÄ‚îÄ app/
‚îÇ       ‚îú‚îÄ‚îÄ main.py        app, lifespan, router wiring
‚îÇ       ‚îú‚îÄ‚îÄ config.py      env vars, model names, pricing
‚îÇ       ‚îú‚îÄ‚îÄ models.py      Pydantic models + LanceDB schema
‚îÇ       ‚îú‚îÄ‚îÄ prompts.py     LLM extraction prompt templates
‚îÇ       ‚îú‚îÄ‚îÄ telemetry.py   CostLedger + ActivityLog
‚îÇ       ‚îú‚îÄ‚îÄ registry.py    NameRegistry (hash ‚Üí display name)
‚îÇ       ‚îú‚îÄ‚îÄ db.py          LanceDB table state
‚îÇ       ‚îú‚îÄ‚îÄ embedder.py    Voyage AI wrapper
‚îÇ       ‚îú‚îÄ‚îÄ extractor.py   xAI client + extraction logic
‚îÇ       ‚îú‚îÄ‚îÄ store.py       dedup, aging rules, CRUD helpers
‚îÇ       ‚îî‚îÄ‚îÄ routes/        memories ¬∑ projects ¬∑ system
‚îú‚îÄ‚îÄ frontend/     Next.js 15 ¬∑ Tailwind ‚Äî web dashboard
‚îî‚îÄ‚îÄ scripts/      start.sh ‚Äî docker compose orchestration helper
```

```mermaid
flowchart TD
    subgraph PLUGIN["üîå OpenCode Plugin ‚Äî Bun ¬∑ TypeScript"]
        H1["chat.message ‚Äî turn 1: populates session cache; turns 2+: semantic refresh"]
        H5["system.transform ‚Äî rebuilds and injects MEMORY into system prompt every LLM call"]
        H2["messages.transform ‚Äî caches full conversation before every LLM call"]
        H3["event: message.updated ‚Äî auto-saves atomic facts after every completed turn"]
        H4["compaction hook ‚Äî preserves memories through context window truncation"]
    end

    PLUGIN -->|"HTTP ¬∑ localhost:8020"| SERVER

    subgraph SERVER["‚öôÔ∏è Memory Server ‚Äî Python ¬∑ FastAPI ¬∑ Docker"]
        subgraph WRITE["POST /memories"]
            direction LR
            W1["xAI Grok<br/>extract typed facts + chunks"] --> W2["Voyage voyage-code-3<br/>embed ¬∑ 1024 dims"] --> W3["LanceDB<br/>cosine dedup ‚Üí upsert"] --> W4["xAI Grok<br/>contradiction detect ‚Üí supersede stale"]
        end
        subgraph FIND["POST /memories/search"]
            direction LR
            F1["Voyage voyage-code-3<br/>embed query"] --> F2["LanceDB<br/>ANN cosine ¬∑ top-k<br/>filter superseded_by = ''"] --> F3["recency blend<br/>(optional weight)"]
        end
        REST["GET /memories ¬∑ /projects ¬∑ /stats ¬∑ /costs ¬∑ /health ¬∑ /activity"]
        VOL[("opencode-memory-data ‚Äî Docker volume ¬∑ Apache Arrow ¬∑ Parquet")]
        WRITE --> VOL
        FIND --> VOL
    end

    DASH -->|"HTTP ¬∑ localhost:8020"| SERVER

    subgraph DASH["üñ•Ô∏è Dashboard ‚Äî Next.js 15 ¬∑ Tailwind ¬∑ localhost:3030"]
        UI["Memory browser ¬∑ Cost tracker ¬∑ Live activity feed"]
    end

    classDef hook fill:#0f172a,stroke:#38bdf8,color:#bae6fd
    classDef ai   fill:#052e16,stroke:#4ade80,color:#bbf7d0
    classDef rest fill:#1c1917,stroke:#fb923c,color:#fed7aa
    classDef db   fill:#1e1b4b,stroke:#a78bfa,color:#ddd6fe
    classDef ui   fill:#2d1b4e,stroke:#c084fc,color:#f3e8ff

    class H1,H2,H3,H4,H5 hook
    class W1,W2,W3,F1,F2 ai
    class REST rest
    class VOL db
    class UI ui
```

---

## Configuration

The plugin reads an optional config file at `~/.config/opencode/memory.jsonc`:

```jsonc
{
  // Memory server URL (default: http://localhost:8020)
  "memoryBaseUrl": "http://localhost:8020",

  // Minimum similarity score for retrieval, 0‚Äì1 (default: 0.45)
  // Higher values cut noise; lower values improve recall on broad queries
  "similarityThreshold": 0.45,

  // Max memories retrieved per scope per session (default: 10)
  // Applied separately to user-scope and project-scope searches, so
  // total semantic memories available = up to 2√ó this value.
  // Benchmark runs at K=20 (project+user combined) ‚Äî this matches that.
  "maxMemories": 10,

  // Context fill ratio that triggers compaction hook (default: 0.80)
  "compactionThreshold": 0.80,

  // Additional keyword patterns that trigger an explicit save (regex strings)
  // "keywordPatterns": ["bookmark this", "save for later"]
}
```

---

## Development

```bash
# Rebuild the TypeScript plugin after making changes
cd plugin
bun run build

# Type-check only (no output)
bun run typecheck
```

```bash
# Rebuild and restart the memory server after backend changes
docker compose build opencode-memory-backend
docker compose up -d opencode-memory-backend
```

**Useful commands:**

```bash
# Tail plugin logs
tail -f ~/.opencode-memory.log

# Tail memory server logs
docker compose logs -f opencode-memory-backend

# Check server health
curl http://localhost:8020/health

# List all projects with memory counts
curl http://localhost:8020/projects

# Check accumulated API costs
curl http://localhost:8020/costs

# Wipe all memories (destructive ‚Äî stops containers and deletes volume)
docker compose down -v
```

---

## Key Engineering Notes

Non-obvious findings from building on the OpenCode plugin API:

1. **`finish` is a string, not a boolean.** Values are `"tool-calls"` (mid-turn) and `"stop"` (turn complete). Checking `finish === true` never matches.

2. **`session.idle` doesn't fire reliably.** Use `message.updated` with `finish === "stop"` for post-turn processing instead.

3. **`ctx.client.session.messages()` always returns 401.** The plugin's HTTP client isn't pre-configured with the OpenCode auth token. Use `experimental.chat.messages.transform` to read conversation messages.

4. **`finish: "stop"` fires twice per turn.** A 15-second cooldown deduplicates it.

5. **Three plugin instances initialise per session.** OpenCode creates one per open directory. They share module-level state via Bun's module system ‚Äî singletons are safe.

6. **Docker Compose shell env vars override `.env`.** If `XAI_API_KEY` is set in your shell, it wins over the file. Use `env_file: - .env` to always load from the file directly.

---

## Stopping and Resetting

```bash
# Stop the server, keep all memories
docker compose down

# Stop and permanently wipe all memories
docker compose down -v
```

---

## Privacy

All data stays on your machine. The only outbound API calls are:

- **Voyage AI** ‚Äî text is sent to generate embeddings. Voyage does not store your data.
- **xAI** ‚Äî conversation text is sent to Grok for memory extraction.

To exclude sensitive content from extraction, wrap it in `<private>...</private>` ‚Äî it will be stripped before any text leaves your machine.

---

## License

[MIT](./LICENSE) ‚Äî use it, fork it, ship it.

---

<div align="center">

Built with [OpenCode](https://opencode.ai) ¬∑ [LanceDB](https://lancedb.com) ¬∑ [Voyage AI](https://www.voyageai.com) ¬∑ [xAI](https://x.ai) ¬∑ [FastAPI](https://fastapi.tiangolo.com) ¬∑ [Bun](https://bun.sh)

</div>
